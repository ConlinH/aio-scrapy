# 中间件 | Middlewares

中间件是AioScrapy的重要组成部分，它们允许您在请求和响应的处理过程中插入自定义代码。AioScrapy提供了两种主要类型的中间件：下载器中间件和爬虫中间件。</br>
Middlewares are an important part of AioScrapy, allowing you to insert custom code into the request and response processing pipeline. AioScrapy provides two main types of middlewares: downloader middlewares and spider middlewares.

## 中间件架构 | Middleware Architecture

AioScrapy的中间件系统基于一个处理链，每个中间件组件都可以修改请求和响应，或者完全中断处理链。</br>
AioScrapy's middleware system is based on a processing chain, where each middleware component can modify requests and responses, or completely interrupt the processing chain.

### 下载器中间件 | Downloader Middleware

下载器中间件位于引擎和下载器之间，可以处理请求在发送到下载器之前和响应在返回到引擎之后的过程。</br>
Downloader middleware sits between the engine and the downloader, processing requests before they are sent to the downloader and responses after they are returned to the engine.

### 爬虫中间件 | Spider Middleware

爬虫中间件位于引擎和爬虫之间，可以处理爬虫生成的请求和下载器返回的响应。</br>
Spider middleware sits between the engine and the spider, processing requests generated by the spider and responses returned by the downloader.

## 下载器中间件 | Downloader Middleware
### 创建下载器中间件 | Creating a Downloader Middleware

下载器中间件是一个Python类，实现了一个或多个以下方法：</br>
A downloader middleware is a Python class that implements one or more of the following methods:

```python
class MyDownloaderMiddleware:

    # 这个方法用于从爬虫创建中间件实例 | This method is used to create middleware instances from a crawler
    @classmethod
    def from_crawler(cls, crawler):
        return cls()

    # 在请求被发送到下载器之前处理请求 | Process the request before it is sent to the downloader
    async def process_request(self, request, spider):
        return None  # 继续处理链 | Continue processing chain

        # 或者 | or
        # return Response(...)  # 返回响应，跳过下载器 | Return a response, skip the downloader

        # 或者 | or
        # return Request(...)  # 返回新请求，替换原请求 | Return a new request, replace the original request

        # 或者 | or
        # raise IgnoreRequest(...)  # 忽略请求，调用process_exception | Ignore the request, call process_exception

    # 在响应返回到引擎之前处理响应 | Process the response before it is returned to the engine
    async def process_response(self, request, response, spider):
        return response  # 返回响应或修改后的响应 | Return the response or a modified response

        # 或者 | or
        # return Request(...)  # 返回新请求，替换响应 | Return a new request, replace the response

        # 或者 | or
        # raise IgnoreRequest(...)  # 忽略响应，调用process_exception | Ignore the response, call process_exception

    # 当下载器或process_request方法抛出异常时处理异常 | Process exceptions raised by the downloader or process_request method
    async def process_exception(self, request, exception, spider):
        return None  # 继续处理异常 | Continue processing the exception

        # 或者 | or
        # return Response(...)  # 返回响应，恢复正常处理 | Return a response, resume normal processing

        # 或者 | or
        # return Request(...)  # 返回新请求，替换原请求 | Return a new request, replace the original request

```

### 启用下载器中间件 | Enabling a Downloader Middleware

要启用下载器中间件，将其添加到项目的`settings.py`文件中的`DOWNLOADER_MIDDLEWARES`设置：</br>
To enable a downloader middleware, add it to the `DOWNLOADER_MIDDLEWARES` setting in your project's `settings.py` file:

```python
DOWNLOADER_MIDDLEWARES = {
    'myproject.middlewares.MyDownloaderMiddleware': 543,
}
```

数字表示中间件的顺序，数字越小，中间件越靠近引擎；数字越大，中间件越靠近下载器。</br>
The number represents the order of the middleware, with lower numbers being closer to the engine and higher numbers being closer to the downloader.

### 内置下载器中间件 | Built-in Downloader Middlewares

AioScrapy包含多个内置的下载器中间件，提供常见功能：</br>
AioScrapy includes several built-in downloader middlewares that provide common functionality:

- **DownloadTimeoutMiddleware**：处理下载超时
- **DefaultHeadersMiddleware**：添加默认请求头
- **UserAgentMiddleware**：设置User-Agent
- **RetryMiddleware**：重试失败的请求
- **DownloaderStats**：收集下载统计信息
- **TLSCiphersMiddleware**：管理TLS指纹

</br>

- **DownloadTimeoutMiddleware**: Handles download timeouts
- **DefaultHeadersMiddleware**: Adds default headers
- **UserAgentMiddleware**: Sets User-Agent
- **RetryMiddleware**: Retries failed requests
- **DownloaderStats**: Collects download statistics
- **TLSCiphersMiddleware**: Manages TLS fingerprints

## 爬虫中间件 | Spider Middleware
### 创建爬虫中间件 | Creating a Spider Middleware

爬虫中间件是一个Python类，实现了一个或多个以下方法：</br>
A spider middleware is a Python class that implements one or more of the following methods:

```python
class MySpiderMiddleware:

    # 这个方法用于从爬虫创建中间件实例 | This method is used to create middleware instances from a crawler
    @classmethod
    def from_crawler(cls, crawler):
        return cls()

    # 在响应传递给爬虫之前处理响应 | Process the response before it is passed to the spider
    async def process_spider_input(self, response, spider):
        return None  # 继续处理链 | Continue processing chain

        # 或者 | or
        # raise IgnoreRequest(...)  # 忽略响应 | Ignore the response

    # 处理爬虫返回的结果 | Process the results returned by the spider
    async def process_spider_output(self, response, result, spider):
        for item_or_request in result:
            yield item_or_request  # 返回原始项或请求，或修改后的项或请求 | Return original or modified items or requests

    # 当爬虫或process_spider_input方法抛出异常时处理异常 | Process exceptions raised by the spider or process_spider_input method
    async def process_spider_exception(self, response, exception, spider):
        return None  # 继续处理异常 | Continue processing the exception

        # 或者 | or
        # return list(items_or_requests)  # 返回项或请求列表，恢复正常处理 | Return a list of items or requests, resume normal processing

    # 处理爬虫的起始请求 | Process the spider's start requests
    async def process_start_requests(self, start_requests, spider):
        for request in start_requests:
            yield request  # 返回原始请求或修改后的请求 | Return original or modified requests

```

### 启用爬虫中间件 | Enabling a Spider Middleware

要启用爬虫中间件，将其添加到项目的`settings.py`文件中的`SPIDER_MIDDLEWARES`设置：</br>
To enable a spider middleware, add it to the `SPIDER_MIDDLEWARES` setting in your project's `settings.py` file:

```python
SPIDER_MIDDLEWARES = {
    'myproject.middlewares.MySpiderMiddleware': 543,
}
```

数字表示中间件的顺序，数字越小，中间件越靠近引擎；数字越大，中间件越靠近爬虫。</br>
The number represents the order of the middleware, with lower numbers being closer to the engine and higher numbers being closer to the spider.

### 内置爬虫中间件 | Built-in Spider Middlewares

AioScrapy包含多个内置的爬虫中间件，提供常见功能：</br>
AioScrapy includes several built-in spider middlewares that provide common functionality:

- **HttpErrorMiddleware**：处理HTTP错误（例如，404、500）
- **OffsiteMiddleware**：过滤掉对爬虫允许的域之外的URL的请求
- **RefererMiddleware**：设置Referer头
- **UrlLengthMiddleware**：过滤掉URL长度超过URLLENGTH_LIMIT的请求
- **DepthMiddleware**：跟踪请求深度

</br>

- **HttpErrorMiddleware**: Handles HTTP errors (e.g., 404, 500)
- **OffsiteMiddleware**: Filters out requests to URLs outside the domains allowed by the spider
- **RefererMiddleware**: Sets the Referer header
- **UrlLengthMiddleware**: Filters out requests with URLs longer than URLLENGTH_LIMIT
- **DepthMiddleware**: Tracks request depth

## 中间件示例 | Middleware Examples
### 自定义User-Agent中间件 | Custom User-Agent Middleware

```python
import random
from aioscrapy import signals
from aioscrapy.middleware.downloader import UserAgentMiddleware

class RandomUserAgentMiddleware(UserAgentMiddleware):
    def __init__(self, user_agents):
        super().__init__()
        self.user_agents = user_agents

    @classmethod
    def from_crawler(cls, crawler):
        user_agents = crawler.settings.getlist('USER_AGENT_LIST')
        if not user_agents:
            user_agents = [
                'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
                'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
                'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:89.0) Gecko/20100101 Firefox/89.0'
            ]
        middleware = cls(user_agents)
        crawler.signals.connect(middleware.spider_opened, signal=signals.spider_opened)
        return middleware

    async def process_request(self, request, spider):
        if not request.headers.get('User-Agent'):
            request.headers['User-Agent'] = random.choice(self.user_agents)
```

### 代理中间件 | Proxy Middleware

```python
from aioscrapy import signals
from aioscrapy.exceptions import IgnoreRequest

class ProxyMiddleware:
    def __init__(self, proxies):
        self.proxies = proxies
        self.current_proxy = None

    @classmethod
    def from_crawler(cls, crawler):
        proxies = crawler.settings.getlist('PROXY_LIST')
        if not proxies:
            raise ValueError("PROXY_LIST setting is empty")
        middleware = cls(proxies)
        crawler.signals.connect(middleware.spider_opened, signal=signals.spider_opened)
        return middleware

    async def spider_opened(self, spider):
        self.current_proxy = self.proxies[0]

    async def process_request(self, request, spider):
        if 'proxy' not in request.meta:
            request.meta['proxy'] = self.current_proxy

    # 如果请求失败，尝试更换代理 | If the request fails, try changing the proxy
    async def process_exception(self, request, exception, spider):
        if 'proxy' in request.meta:
            current_proxy = request.meta['proxy']
            if current_proxy in self.proxies:
                index = self.proxies.index(current_proxy)
                next_index = (index + 1) % len(self.proxies)
                self.current_proxy = self.proxies[next_index]
                
                # 创建新请求，使用新代理 | Create a new request with the new proxy
                request.meta['proxy'] = self.current_proxy
                return request
```

## 最佳实践 | Best Practices

1. **保持中间件简单**：每个中间件应该只关注一个功能
2. **正确处理异常**：确保在`process_exception`方法中正确处理异常
3. **注意中间件顺序**：中间件的执行顺序很重要，确保它们按照预期的顺序执行
4. **使用`from_crawler`方法**：使用`from_crawler`方法从爬虫获取设置和信号
5. **避免阻塞操作**：所有中间件方法都应该是异步的，避免阻塞操作

</br>

1. **Keep Middlewares Simple**: Each middleware should focus on a single functionality
2. **Handle Exceptions Properly**: Make sure to handle exceptions correctly in the `process_exception` method
3. **Pay Attention to Middleware Order**: The execution order of middlewares is important, ensure they execute in the expected order
4. **Use the `from_crawler` Method**: Use the `from_crawler` method to get settings and signals from the crawler
5. **Avoid Blocking Operations**: All middleware methods should be asynchronous, avoid blocking operations
